"""
How do you test a test framework?

You can't use the test framework to test itself, because it may contain bugs!
Hence this script, which uses upytest to run tests and check the results are as
expected. The expected results are hard-coded in this script, and the actual
results are generated by running tests with upytest. The script then compares
the expected and actual results to ensure they match.

Finally, the script creates a div element to display the results in the page.
If tests fail, the script will raise an AssertionError, which will be
displayed with a red background. If the tests pass, the script will display a
message with a green background.

There are two sorts of expected results: the number of tests that pass, fail,
and are skipped, and the names of the tests that pass, fail, and are skipped.
Tests that pass end with "passes", tests that fail end with "fails", and tests
that are skipped end with "skipped".

This script will work with both MicroPython and Pyodide, just so we can ensure
the test framework works in both environments. The index.html file uses
MicroPython, the index2.html file uses Pyodide.

That's it! Now we can test a test framework with a meta-test framework. ðŸ¤¯
"""

from pyscript.web import page, div, h2, p, b
import upytest


expected_results = {
    "result_all": {
        "passes": 8,
        "fails": 6,
        "skipped": 4,
    },
    "result_module": {
        "passes": 7,
        "fails": 6,
        "skipped": 4,
    },
    "result_specific": {
        "passes": 1,
        "fails": 0,
        "skipped": 0,
    },
}

actual_results = {}
# Run all tests in the tests directory.
actual_results["result_all"] = await upytest.run("./tests")
# Run all tests in a specific module.
actual_results["result_module"] = await upytest.run(
    "tests/test_core_functionality.py"
)
# Run a specific test function.
actual_results["result_specific"] = await upytest.run(
    "tests/test_core_functionality.py::test_passes"
)

# Evaluate the results have the right number of tests.
for name, result in expected_results.items():
    for key, value in result.items():
        actual = len(actual_results[name][key])
        assert (
            actual == value
        ), f"Expected {value} {key} in {name}, got {actual}"

# Ensure the tests that pass have a name ending in "passed", tests that fail
# have a name ending in "failed", and tests that are skipped have a name ending
# in "skipped".
for name, result in actual_results.items():
    for key, value in result.items():
        for test in value:
            assert test.endswith(key), f"Test {test} does not end with {key}"

# Create a div to display the results in the page.
page.append(
    div(
        h2("Test Results All Correct âœ¨"),
        div(
            p(
                b("All Tests: "),
                f"Passes: {len(actual_results['result_all']['passes'])},"
                f" Fails: {len(actual_results['result_all']['fails'])},"
                f" Skipped: {len(actual_results['result_all']['skipped'])}.",
            ),
        ),
        div(
            p(
                b("Tests in a Specified Module: "),
                f"Passes: {len(actual_results['result_module']['passes'])},"
                f" Fails: {len(actual_results['result_module']['fails'])},"
                f" Skipped: {len(actual_results['result_module']['skipped'])}.",
            ),
        ),
        div(
            p(
                b("Test a Specific Test: "),
                f"Passes: {len(actual_results['result_specific']['passes'])},"
                f" Fails: {len(actual_results['result_specific']['fails'])},"
                f" Skipped: {len(actual_results['result_specific']['skipped'])}.",
            ),
        ),
        style={
            "background-color": "lightgreen",
            "padding": "10px",
            "border": "1px solid green",
        },
    )
)
